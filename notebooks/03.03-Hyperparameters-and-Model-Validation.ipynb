{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/header_small.png\">\n",
    "\n",
    "*Esta libreta contiene material del Taller de Python que se lleva a cabo como parte del \n",
    "evento [Data Challenge Industrial 4.0](www.lania.mx/dci). El contenido ha sido adaptado \n",
    "por HTM y GED a partir del libro [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) \n",
    "de Jake VanderPlas y se mantiene la licencia sobre el texto, \n",
    "[CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), \n",
    "y sobre el codigo [MIT license](https://opensource.org/licenses/MIT).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Introduciendo Scikit-Learn](03.02-Introducing-Scikit-Learn.ipynb) | [Contenido](Index.ipynb) | [Ingeniería de Atributos](03.04-Feature-Engineering.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/htapia/TallerPythonIntroCienciaDatos/blob/master/notebooks/03.03-Hyperparameters-and-Model-Validation.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparámetros y validación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como no hacer una validación del modelo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "y_model = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forma correcta para la validación del modelo: conjuntos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-600058012b06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# split the data with 50% in each set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n\u001b[0;32m      4\u001b[0m                                   train_size=0.5)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n",
    "                                  train_size=0.5)\n",
    "\n",
    "# fit the model on one set of data\n",
    "model.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y2_model = model.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación cuzada de modelos (cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_model = model.fit(X1, y1).predict(X2)\n",
    "y1_model = model.fit(X2, y2).predict(X1)\n",
    "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hay una representación visual de la validación cruzada de cinco veces:\n",
    "\n",
    "![](figures/05.03-5-fold-CV.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import LeaveOneOut\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other cross-validation schemes can be used similarly.\n",
    "For a description of what is available in Scikit-Learn, use IPython to explore the ``sklearn.cross_validation`` submodule, or take a look at Scikit-Learn's online [cross-validation documentation](http://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "Otro esquema de validación cruzada puede ser usada de manera similar.\n",
    "Para obtener una descripción de lo que está disponible en Scikit-Learn, use IPython para explorar el submódulo `` sklearn.cross_validation``, o eche un vistazo a la [documentación de validación cruzada] en línea de Scikit-Learn (http://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección del mejor modelo\n",
    "\n",
    "Of core importance is the following question: *if our estimator is underperforming, how should we move forward?*\n",
    "There are several possible answers:\n",
    "\n",
    "- Use a more complicated/more flexible model\n",
    "- Use a less complicated/less flexible model\n",
    "- Gather more training samples\n",
    "- Gather more data to add features to each sample\n",
    "\n",
    "The answer to this question is often counter-intuitive.\n",
    "In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results!\n",
    "The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful.\n",
    "\n",
    "\n",
    "De importancia fundamental es la siguiente pregunta: si nuestro estimador tiene un rendimiento inferior, ¿cómo debemos avanzar? Hay varias respuestas posibles:\n",
    "\n",
    "     Use un modelo más complicado / más flexible\n",
    "     Use un modelo menos complicado / menos flexible\n",
    "     Reúne más muestras de entrenamiento\n",
    "     Reúna más datos para agregar características a cada muestra\n",
    "\n",
    "La respuesta a esta pregunta es a menudo contra intuitiva. En particular, a veces usar un modelo más complicado dará peores resultados, ¡y agregar más muestras de entrenamiento puede no mejorar sus resultados! La capacidad de determinar qué pasos mejorarán su modelo es lo que separa a los profesionales exitosos de aprendizaje automático de los que no tuvieron éxito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compensando entre la varianza y la tendencia\n",
    "\n",
    "Fundamentally, the question of \"the best model\" is about finding a sweet spot in the tradeoff between *bias* and *variance*.\n",
    "Consider the following figure, which presents two regression fits to the same dataset:\n",
    "\n",
    "![](figures/05.03-bias-variance.png)\n",
    "\n",
    "It is clear that neither of these models is a particularly good fit to the data, but they fail in different ways.\n",
    "\n",
    "The model on the left attempts to find a straight-line fit through the data.\n",
    "Because the data are intrinsically more complicated than a straight line, the straight-line model will never be able to describe this dataset well.\n",
    "Such a model is said to *underfit* the data: that is, it does not have enough model flexibility to suitably account for all the features in the data; another way of saying this is that the model has high *bias*.\n",
    "\n",
    "The model on the right attempts to fit a high-order polynomial through the data.\n",
    "Here the model fit has enough flexibility to nearly perfectly account for the fine features in the data, but even though it very accurately describes the training data, its precise form seems to be more reflective of the particular noise properties of the data rather than the intrinsic properties of whatever process generated that data.\n",
    "Such a model is said to *overfit* the data: that is, it has so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution; another way of saying this is that the model has high *variance*.\n",
    "\n",
    "Está claro que ninguno de estos modelos se ajusta particularmente bien a los datos, pero fallan de diferentes maneras.\n",
    "\n",
    "El modelo de la izquierda intenta encontrar un ajuste en línea recta a través de los datos. Debido a que los datos son intrínsecamente más complicados que una línea recta, el modelo de línea recta nunca podrá describir bien este conjunto de datos. Se dice que un modelo de este tipo no ajusta los datos: es decir, no tiene suficiente flexibilidad del modelo para dar cuenta de manera adecuada de todas las características de los datos; Otra forma de decir esto es que el modelo tiene un alto sesgo.\n",
    "\n",
    "El modelo de la derecha intenta ajustar un polinomio de alto orden a través de los datos. Aquí, el ajuste del modelo tiene suficiente flexibilidad para dar cuenta casi a la perfección de las características finas en los datos, pero a pesar de que describe con mucha precisión los datos de entrenamiento, su forma precisa parece reflejar más las propiedades de ruido particulares de los datos en lugar de lo intrínseco. propiedades de cualquier proceso que haya generado esos datos. Se dice que dicho modelo sobreajusta los datos: es decir, tiene tanta flexibilidad que el modelo termina teniendo en cuenta los errores aleatorios, así como la distribución de datos subyacente; Otra forma de decir esto es que el modelo tiene una gran *varianza*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver esto desde otra perspectiva, considere lo que sucede sí usamos estos dos modelos para predecir el valor de y para algunos datos nuevos.\n",
    "En los siguientes diagramas, los puntos rojo / más claro indican datos que se omiten del conjunto de entrenamiento:\n",
    "\n",
    "![](figures/05.03-bias-variance-2.png)\n",
    "\n",
    "La puntuación aquí es la puntuación $R^2$, o [coeficiente de determinación] (https://en.wikipedia.org/wiki/Coefficient_of_determination), que mide qué tan bien se desempeña un modelo en relación con una media simple de los valores objetivo . $R^2=1$ indica una coincidencia perfecta, $R^2=0$ indica que el modelo no es mejor que simplemente tomar la media de los datos, y los valores negativos significan modelos aún peores.\n",
    "A partir de los puntajes asociados con estos dos modelos, podemos hacer una observación más general:\n",
    "\n",
    "- Para los modelos de alto sesgo, el rendimiento del modelo en el conjunto de validación es similar al rendimiento en el conjunto de entrenamiento.\n",
    "- Para los modelos de alta varianza, el rendimiento del modelo en el conjunto de validación es mucho peor que el rendimiento en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sí imaginamos que tenemos alguna capacidad para ajustar la complejidad del modelo, esperaríamos que el puntaje de entrenamiento y el puntaje de validación se comporten como se ilustra en la siguiente figura:\n",
    "\n",
    "![](figures/05.03-validation-curve.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Validation-Curve)\n",
    "\n",
    "El diagrama que se muestra aquí a menudo se llama * curva de validación *, y vemos las siguientes características esenciales:\n",
    "\n",
    "- El puntaje de entrenamiento es en todas partes más alto que el puntaje de validación. Este es generalmente el caso: el modelo se ajustará mejor a los datos que ha visto que a los datos que no ha visto.\n",
    "- Para una complejidad de modelo muy baja (un modelo de alto sesgo), los datos de entrenamiento no están ajustados, lo que significa que el modelo es un mal predictor tanto para los datos de entrenamiento como para los datos no vistos previamente.\n",
    "- Para una complejidad de modelo muy alta (un modelo de alta varianza), los datos de entrenamiento están sobreajustados, lo que significa que el modelo predice muy bien los datos de entrenamiento, pero falla para cualquier dato no visto previamente.\n",
    "- Para algunos valores intermedios, la curva de validación tiene un máximo. Este nivel de complejidad indica una compensación adecuada entre sesgo y varianza.\n",
    "\n",
    "Los medios para ajustar la complejidad del modelo varían de un modelo a otro; Cuando analicemos los modelos individuales en profundidad en secciones posteriores, veremos cómo cada modelo permite tal ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Curvas de validación en Scikit-Learn\n",
    "\n",
    "Veamos un ejemplo de uso de validación cruzada para calcular la curva de validación para una clase de modelos.\n",
    "Aquí usaremos un modelo de * regresión polinómica *: este es un modelo lineal generalizado en el cual el grado del polinomio es un parámetro ajustable.\n",
    "Por ejemplo, un polinomio de grado 1 ajusta una línea recta a los datos; para los parámetros del modelo $ a $ y $ b $:\n",
    "\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "\n",
    "Un polinomio de grado 3 ajusta una curva cúbica a los datos; para los parámetros del modelo $ a, b, c, d $:\n",
    "\n",
    "$$\n",
    "y = ax ^ 3 + bx ^ 2 + cx + d\n",
    "$$\n",
    "\n",
    "Podemos generalizar esto a cualquier número de características polinómicas.\n",
    "En Scikit-Learn, podemos implementar esto con una regresión lineal simple combinada con el preprocesador polinómico.\n",
    "Usaremos una * tubería * para unir estas operaciones (discutiremos las características y tuberías polinomiales más detalladamente en [Ingeniería de características] (05.04-Feature-Engineering.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1. / (X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # plot formatting\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color='black')\n",
    "axis = plt.axis()\n",
    "for degree in [1, 3, 5]:\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "degree = np.arange(0, 21)\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n",
    "                                          'polynomialfeatures__degree', degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = PolynomialRegression(3).fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test);\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvas de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = make_data(200)\n",
    "plt.scatter(X2.ravel(), y2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = np.arange(21)\n",
    "train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,\n",
    "                                            'polynomialfeatures__degree', degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score2, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3, linestyle='dashed')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed')\n",
    "plt.legend(loc='lower center')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las líneas continuas muestran los nuevos resultados, mientras que las líneas discontinuas más débiles muestran los resultados del conjunto de datos anterior más pequeño.\n",
    "A partir de la curva de validación, es claro que el conjunto de datos más grande puede admitir un modelo mucho más complicado: el pico aquí es probablemente de alrededor de un grado de 6, pero incluso un modelo de grado 20 no está ajustando demasiado los datos: la validación y la capacitación los puntajes permanecen muy cerca.\n",
    "\n",
    "Por lo tanto, vemos que el comportamiento de la curva de validación no tiene una sino dos entradas importantes: la complejidad del modelo y el número de puntos de entrenamiento.\n",
    "A menudo es útil explorar el comportamiento del modelo en función del número de puntos de entrenamiento, lo que podemos hacer utilizando subconjuntos de datos cada vez más grandes para adaptarnos a nuestro modelo.\n",
    "Una gráfica del puntaje de entrenamiento / validación con respecto al tamaño del conjunto de entrenamiento se conoce como * curva de aprendizaje. *\n",
    "\n",
    "El comportamiento general que esperaríamos de una curva de aprendizaje es este:\n",
    "\n",
    "- Un modelo de una complejidad dada * sobreajustará * un pequeño conjunto de datos: esto significa que el puntaje de entrenamiento será relativamente alto, mientras que el puntaje de validación será relativamente bajo.\n",
    "- Un modelo de una complejidad dada * subestimará * un gran conjunto de datos: esto significa que el puntaje de entrenamiento disminuirá, pero el puntaje de validación aumentará.\n",
    "- Un modelo nunca, excepto por casualidad, dará una mejor puntuación al conjunto de validación que al conjunto de entrenamiento: esto significa que las curvas deben seguir acercándose pero nunca cruzarse.\n",
    "\n",
    "Con estas características en mente, esperaríamos que una curva de aprendizaje se vea cualitativamente como la que se muestra en la siguiente figura:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.03-learning-curve.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Learning-Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La característica notable de la curva de aprendizaje es la convergencia a un puntaje particular a medida que crece el número de muestras de entrenamiento.\n",
    "En particular, una vez que tenga suficientes puntos para que un modelo en particular haya convergido, * ¡agregar más datos de entrenamiento no lo ayudará! *\n",
    "La única forma de aumentar el rendimiento del modelo en este caso es usar otro modelo (a menudo más complejo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvas de aprendizaje en Scikit-Learn\n",
    "\n",
    "La característica notable de la curva de aprendizaje es la convergencia a un puntaje particular a medida que crece el número de muestras de entrenamiento.\n",
    "En particular, una vez que tenga suficientes puntos para que un modelo en particular haya convergido, * ¡agregar más datos de entrenamiento no lo ayudará! *\n",
    "La única forma de aumentar el rendimiento del modelo en este caso es usar otro modelo (a menudo más complejo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for i, degree in enumerate([2, 9]):\n",
    "    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),\n",
    "                                         X, y, cv=7,\n",
    "                                         train_sizes=np.linspace(0.3, 1, 25))\n",
    "\n",
    "    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
    "    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
    "    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n",
    "                 color='gray', linestyle='dashed')\n",
    "\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_xlim(N[0], N[-1])\n",
    "    ax[i].set_xlabel('training size')\n",
    "    ax[i].set_ylabel('score')\n",
    "    ax[i].set_title('degree = {0}'.format(degree), size=14)\n",
    "    ax[i].legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación en la práctica: Grid Search\n",
    "\n",
    "La discusión anterior tiene la intención de darle una cierta intuición sobre el equilibrio entre sesgo y varianza, y su dependencia de la complejidad del modelo y el tamaño del conjunto de entrenamiento.\n",
    "En la práctica, los modelos generalmente tienen más de una perilla para girar y, por lo tanto, las curvas de validación y las curvas de aprendizaje cambian de líneas a superficies multidimensionales.\n",
    "En estos casos, tales visualizaciones son difíciles y preferimos simplemente encontrar el modelo particular que maximice la puntuación de validación.\n",
    "\n",
    "Scikit-Learn proporciona herramientas automatizadas para hacer esto en el módulo de búsqueda de cuadrícula.\n",
    "Aquí hay un ejemplo del uso de la búsqueda de cuadrícula para encontrar el modelo polinómico óptimo.\n",
    "Exploraremos una cuadrícula tridimensional de características del modelo; a saber, el grado polinomial, la bandera que nos indica si debemos ajustar la intersección, y la bandera que nos dice si debemos normalizar el problema.\n",
    "Esto se puede configurar utilizando el metaestimulador `` GridSearchCV`` de Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
    "              'linearregression__fit_intercept': [True, False],\n",
    "              'linearregression__normalize': [True, False]}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = model.fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test, hold=True);\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "En ésta sección exploramos el concepto de validación de modelo y optimización de hiperparámetros, enfocados en los aspectos de tendencia y varianza de los datos y su efecto en el modelado. El uso de conjuntos de validación o validación cruzada son *fundamentales* para obtener los mejores parámetros y evitar el efecto de un sobre ajuste del modelo, lo cual permite explorar el uso de modelos más complejos o flexibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Introduciendo Scikit-Learn](03.02-Introducing-Scikit-Learn.ipynb) | [Contenido](Index.ipynb) | [Ingeniería de Atributos](03.04-Feature-Engineering.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/htapia/TallerPythonIntroCienciaDatos/blob/master/notebooks/03.03-Hyperparameters-and-Model-Validation.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
